{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hjp__3bRh42K"
   },
   "source": [
    "## Fall 2023 CS 4641/7641 A: Machine Learning Homework 4\n",
    "\n",
    "## Instructor: Dr. Mahdi Roozbahani\n",
    "\n",
    "## Deadline: Friday, December 1, 2023 11:59 pm EST\n",
    "\n",
    "<!-- No changes needed on the below section -->\n",
    "* No unapproved extension of the deadline is allowed. Submission past our 48-hour penalized acceptance period will lead to 0 credit. \n",
    "\n",
    "* Discussion is encouraged on Ed as part of the Q/A. However, all assignments should be done individually.\n",
    "<font color='darkred'>\n",
    "* Plagiarism is a **serious offense**. You are responsible for completing your own work. You are not allowed to copy and paste, or paraphrase, or submit materials created or published by others, as if you created the materials. All materials submitted must be your own.</font>\n",
    "<font color='darkred'>\n",
    "* All incidents of suspected dishonesty, plagiarism, or violations of the Georgia Tech Honor Code will be subject to the instituteâ€™s Academic Integrity procedures. If we observe any (even small) similarities/plagiarisms detected by Gradescope or our TAs, **WE WILL DIRECTLY REPORT ALL CASES TO OSI**, which may, unfortunately, lead to a very harsh outcome. **Consequences can be severe, e.g., academic probation or dismissal, grade penalties, a 0 grade for assignments concerned, and prohibition from withdrawing from the class.**\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wSJJRXYL2yOY"
   },
   "source": [
    "## Instructions for the assignment \n",
    "\n",
    "<!-- No changes needed on the below section -->\n",
    "- This assignment consists of both programming and theory questions.\n",
    "\n",
    "- Unless a theory question explicitly states that no work is required to be shown,  you must provide an explanation, justification, or calculation for your answer.\n",
    "\n",
    "- To switch between cell for code and for markdown, see the menu -> Cell -> Cell Type\n",
    "    \n",
    "- You can directly type Latex equations into markdown cells.\n",
    "    \n",
    "- If a question requires a picture, you could use this syntax `<img src=\"\" style=\"width: 300px;\"/>` to include them within your ipython notebook.\n",
    "\n",
    "- Your write up must be submitted in PDF form. You may use either Latex,  markdown, or any word processing software. <font color = 'darkred'>We will **NOT** accept handwritten work. </font> Make sure that your work is formatted correctly, for example submit $\\sum_{i=0} x_i$ instead of \\text{sum\\_\\{i=0\\} x\\_i}\n",
    "- When submitting the non-programming part of your assignment, you must correctly map pages of your PDF to each question/subquestion to reflect where they appear. <font color='darkred'>**Improperly mapped questions may not be graded correctly and/or will result in point deductions for the error.**</font>\n",
    "- All assignments should be done individually, and each student must write up and submit their own answers.\n",
    "- **Graduate Students**: You are required to complete any sections marked as Bonus for Undergrads  \n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nN-9x-Qm2yOY"
   },
   "source": [
    "## Using the autograder\n",
    "\n",
    "\n",
    "* You will find three assignments (for grads) on Gradescope that correspond to HW4: \"Assignment 4 Programming\", \"Assignment 4 - Non-programming\" and \"Assignment 4 Programming - Bonus for all\". Undergrads will have an additional assignment called \"Assignment 4 Programming - Bonus for Undergrads\". \n",
    "* You will submit your code for the autograder in the Assignment 4 Programming sections. Please refer to the Deliverables and Point Distribution section for what parts are considered required, bonus for undergrads, and bonus for all\".\n",
    "* We provided you different .py files and we added libraries in those files please DO NOT remove those lines and add your code after those lines. Note that these are the only allowed libraries that you can use for the homework \n",
    "* You are allowed to make as many submissions until the deadline as you like. Additionally, note that the autograder tests each function separately, therefore it can serve as a useful tool to help you debug your code if you are not sure of what part of your implementation might have an issue \n",
    "* **For the \"Assignment 4 - Non-programming\" part, you will need to submit to Gradescope a PDF copy of your Jupyter Notebook with the cells ran. [See this EdStem Post for multiple ways on to convert your .ipynb into a .pdf file.](https://edstem.org/us/courses/32639/discussion/2556131)** Please refer to the **Deliverables and Point Distribution** section for an outline of the non-programming questions.\n",
    "* **When submitting to Gradescope, please make sure to mark the page(s) corresponding to each problem/sub-problem. The pages in the PDF should be of size 8.5\" x 11\", otherwise there may be a deduction in points for extra long sheets.**\n",
    "* You **MUST** pass the Autograder Test to gain points for the programming section. There will not be any partial credit or manual grading for this part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "609MA8lGdPzV"
   },
   "source": [
    "## Using the local tests <a id='using_local_tests'></a>\n",
    "- For some of the programming questions we have included a local test using a small toy dataset to aid in debugging. The local test sample data and outputs are stored in localtests.py\n",
    "- There are no points associated with passing or failing the local tests, you must still pass the autograder to get points. \n",
    "- **It is possible to fail the local test and pass the autograder** since the autograder has a certain allowed error tolerance while the local test allowed error may be smaller. Likewise, passing the local tests does not guarantee passing the autograder. \n",
    "- **You do not need to pass both local and autograder tests to get points, passing the Gradescope autograder is sufficient for credit.**\n",
    "- It might be helpful to comment out the tests for functions that have not been completed yet. \n",
    "- It is recommended to test the functions as it gets completed instead of completing the whole class and then testing. This may help in isolating errors. Do not solely rely on the local tests, continue to test on the autograder regularly as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N09bYK1-MnO1"
   },
   "source": [
    "## Deliverables and Points Distribution \n",
    "\n",
    "### Q1: Classification with Two Layer NN [80 pts; 55pts + 25pts Undergrad Bonus] \n",
    "#### Deliverables: <font color = 'green'>NN.py and Notebook Graphs</font>\n",
    "\n",
    "\n",
    "- **1.1 NN Implementation** [65pts; 50pts + 15pts **Bonus for Undergrad**] - _programming_\n",
    "\n",
    "    - Leaky_relu [5pts]\n",
    "\n",
    "    - Softmax [5pts]\n",
    "\n",
    "    - Cross Entropy loss [5pts]\n",
    "\n",
    "    - dropout [5pts]\n",
    "\n",
    "    - forward propagation and with and without dropout [5pts + 5pts]\n",
    "    \n",
    "    - compute gradients and update weights [2.5pts + 2.5pts]\n",
    "\n",
    "    - backward without momentum [5pt]\n",
    "    \n",
    "    - Gradient Descent [10pts]\n",
    "    \n",
    "    - Batch Gradient Descent [10pts **Bonus for Undergrad**]\n",
    "\n",
    "    - Momentum [5pts **Bonus for Undergrad**]\n",
    "\n",
    "- **1.2 Loss plot and CE for Gradient Descent** [5pts] - _non-programming_\n",
    "\n",
    "- **1.3 Loss plot and CE for Batch Gradient Descent** [5pts **Bonus for Undergrad**] - _non-programming_\n",
    "\n",
    "- **1.4 Loss plot and CE value for NN with Gradient Descent with Momentum** [5pts **Bonus for Undergrad**] - _non-programming_\n",
    "\n",
    "\n",
    "### Q2: CNN [25pts; 20pts Bonus for Undergrad + 5pts Bonus for All]\n",
    "#### Deliverables: <font color = 'green'>cnn.py and Written Report</font>\n",
    "\n",
    "- **2.1 Image Classification using Keras CNN** [20pts **Bonus for Undergrad**]\n",
    "\n",
    "    - 2.1.1 Loading the Model [5pts **Bonus for Undergrad**] - *programming*\n",
    "    \n",
    "    - 2.1.3 Building the Model [5pts **Bonus for Undergrad**] - *non-programming*\n",
    "\n",
    "    - 2.1.4 Training the Model [8pts **Bonus for Undergrad**] - *non-programming*\n",
    "\n",
    "    - 2.1.5 Examining Accuracy and Loss [2pts **Bonus for Undergrad**] - *non-programming*\n",
    "\n",
    "- **2.2 Exploring Deep CNN Architectures** [5pts **Bonus for All**] - *non-programming*\n",
    "\n",
    "### Q3: Random Forest [45pts; 40pts + 5pts Bonus for All]\n",
    "#### Deliverables: <font color = 'green'>random_forest.py and Written Report</font>\n",
    "\n",
    "- 3.1 Random Forest Implementation [35pts] - *programming*\n",
    "\n",
    "- 3.2 Hyperparameter Tuning with a Random Forest [5pts] - *programming*\n",
    "\n",
    "- 3.3 Plotting Feature Importance [5pts **Bonus for All**] - *non-programming*\n",
    "\n",
    "### Q4: SVM [34pts Bonus for all]\n",
    "#### Deliverables: <font color = 'green'>feature.py and Written Report</font>\n",
    "\n",
    "- 4.1: Fitting an SVM Classifier by hand [24pts] - *non programming*\n",
    "\n",
    "- 4.2: Feature Mapping [10pts] - *programming*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7U0WVt07tGRv"
   },
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o0Ui6T2as9iI",
    "outputId": "d9a440f2-4324-48ea-d869-5c575b4a565f"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import matplotlib\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_diabetes, fetch_california_housing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from math import log2, sqrt\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import svm\n",
    "from NN import NeuralNet\n",
    "\n",
    "from utilities.utils import get_housing_dataset\n",
    "\n",
    "print('Version information')\n",
    "\n",
    "print('python: {}'.format(sys.version))\n",
    "print('matplotlib: {}'.format(matplotlib.__version__))\n",
    "print('numpy: {}'.format(np.__version__))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mq4QZ4su2yOd"
   },
   "source": [
    "# 1: Two Layer Neural Network [80 pts; 55pts + 25pts Undergrad Bonus] <span style=\"color:blue\">**[P]**</span><span style=\"color:green\">**[W]**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NiDi0B5ZMnO9"
   },
   "source": [
    "## 1.1 NN Implementation [65pts; 50pts + 15pts Bonus for Undergrad] <span style=\"color:blue\">**[P]**</span> \n",
    "\n",
    "In this section, you will implement a two layer fully connected neural network to perform a Classification Task. You will also experiment with different activation functions and optimization techniques. We provide two activation functions here - Leaky Relu and Softmax. You will implement a neural network where the first hidden layer has a Leaky Relu activation and the second hidden layer leads to a Softmax. \n",
    "\n",
    "You'll also implement Gradient Descent (GD) and Batch Gradient Descent (BGD) algorithms for training these neural nets. **GD is mandatory for all. BGD is bonus for undergraduate students but mandatory for graduate students.** \n",
    "\n",
    "In the <strong>NN.py</strong> file, complete the following functions:\n",
    "  * <strong>leaky_relu</strong>\n",
    "  * <strong>softmax</strong>\n",
    "  * <strong>cross_entropy_loss</strong>\n",
    "  * <strong>_dropout</strong>\n",
    "  * <strong>forward</strong>\n",
    "  * <strong>compute_gradients</strong>\n",
    "  * <strong>update_weights</strong>\n",
    "  * <strong>backward</strong>: Note Hint 2, if you still have issues passing the autograder make sure to address Hint 1 as well.\n",
    "  * <strong>gradient_descent</strong>\n",
    "  * <strong>batch_gradient_descent</strong>:<span style=\"color:darkred\"> **Mandatory for graduate students, bonus for undergraduate students.**</span> Please batch your data in a wraparound manner. For example, given a dataset of 9 numbers, [1, 2, 3, 4, 5, 6, 7, 8, 9], and a batch size of 6, the first iteration batch will be [1, 2, 3, 4, 5, 6], the second iteration batch will be [7, 8, 9, 1, 2, 3], the third iteration batch will be [4, 5, 6, 7, 8, 9], etc... \n",
    "  \n",
    "We'll train this neural net on sklearn's California Housing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Function \n",
    "There are many activation functions that are used for various purposes. For this question, we use leaky ReLU and the softmax activation functions. We encourage you to explore the plethora of options, many of which are listed on [Wikipedia](https://en.wikipedia.org/wiki/Activation_function)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid\n",
    "The sigmoid function is a non-linear function with an S-shaped curve and is regarded as a foundational activation function. Its output is in the range $(0, 1)$, making it the function to use for binary classification output. The function is expressed as $$o = \\phi(u)=\\frac{1}{1+e^{-u}}$$<br> The derivation of the sigmoid function is given by $$o' = \\phi'(u) = \\frac{1}{1+e^{-u}} \\left(1-\\frac{1}{1+e^{-u}}\\right) = o(1-o)$$\n",
    "\n",
    "<b>Note:</b> We do not use sigmoid in this homework; it is only included for the sake of completeness. \n",
    "\n",
    "![sigmoid](data/images/sigmoid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "Softmax is a common activation function used in neural networks, especially for multiclass classification problems like the one we are tackling. It is used to convert a vector of raw outputs from the last layer of the Neural Network into a probability distribution over multiple classes. The softmax function takes as input a vector of real numbers and transforms them into a probability distribution, ensuring that the probabilities sum to 1.\n",
    "\n",
    "Mathematically, given an input vector of [x1, x2, ..., xn], the softmax function calculates the probability p(y=i) for each class i as follows:\n",
    "\n",
    "p(y=i) = $e^{xi} / (e^{x1} + e^{x2} + ... + e^{xn})$\n",
    "\n",
    "![sigmoid](data/images/softmax.png)\n",
    "\n",
    "As discussed in class, the equation that we will use in this Neural network accounts for both the x values and the weights: \n",
    "\n",
    "![sigmoid](data/images/softmaxNew.jpg)\n",
    "\n",
    "\n",
    "\n",
    "<strong>TODO:</strong> Implement the function <strong>softmax</strong> in <strong>NN.py</strong>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.localtests import TestNN\n",
    "\n",
    "TestNN('test_softmax').test_softmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU and Leaky ReLU\n",
    "The rectified linear unit (ReLU) is the most commonly used activation function in deep learning today. It takes the form $$o = \\phi(u) = \\max(0,u).$$ Note that ReLU can be computed very quickly due to its simplicity. The derivative of ReLU is given by $$o' = \\phi'(u) = \\begin{cases}\n",
    "0& u \\leq 0 \\\\\n",
    "1& u > 0\n",
    "\\end{cases}.$$\n",
    "\n",
    "![ReLU](data/images/relu.png)\n",
    "\n",
    "Unfortunately, ReLU loses information for negative inputs; it always returns zero. For this reason, some researchers use a variant called leaky ReLU. Unlike ReLU, its leaky counterpart has a small slope (such as $\\alpha = 0.05$) for negative inputs instead of a flat slope.\n",
    "\n",
    "It takes the form\n",
    "$$o = \\phi(u) = \\begin{cases}\n",
    "    \\alpha u & u \\leq 0 \\\\\n",
    "    u & u > 0\n",
    "\\end{cases} $$\n",
    "\n",
    "In this homework, we implement Leaky ReLU.\n",
    "\n",
    "![Leaky ReLU](data/images/leaky_relu.png)\n",
    "\n",
    "<strong>TODO:</strong> Implement the function <strong>leaky_relu</strong> in <strong>NN.py</strong>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.localtests import TestNN\n",
    "\n",
    "TestNN('test_leaky_relu').test_leaky_relu()\n",
    "TestNN('test_d_leaky_relu').test_d_leaky_relu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron\n",
    "A single layer perceptron can be thought of as a linear hyperplane as in logistic regression followed by a non-linear activation function. $$u_{i} = \\sum \\limits_{j=1}^{d} \\theta_{ij}x_{j}+b_{i}$$  $$o_{i} = \\phi \\left( \\sum \\limits_{j=1}^{d} \\theta_{ij}x_{j}+b_{i} \\right) = \\phi(\\theta_{i}^{T}x+b_{i})$$ where $x$ is a d-dimensional vector i.e. $x \\in R^{d}$. It is one datapoint with $d$ features. $\\theta_{i} \\in R^{d}$ is the weight vector for the $i^{th}$ hidden unit, $b_{i} \\in R$ is the bias element for the $i^{th}$ hidden unit and $\\phi(.)$ is a non-linear activation function that has been described below. $u_{i}$ is a linear combination of the features in $x_j$ weighted by $\\theta_{i}$ whereas $o_{i}$ is the $i^{th}$ output unit from the activation layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully connected Layer\n",
    "Typically, a modern neural network contains millions of perceptrons as the one shown in the previous image. Perceptrons interact in different configurations such as cascaded or parallel. In this part, we describe a fully connected layer configuration in a neural network which comprises multiple parallel perceptrons forming one layer. \n",
    "\n",
    "We extend the previous notation to describe a fully connected layer. Each layer in a fully connected network has a number of input/hidden/output units cascaded in parallel. Let us a define a single layer of the neural net as follows: <br>\n",
    "$m$ denotes the number of hidden units in a single layer $l$ whereas $n$ denotes the number of units in the previous layer $l-1$.\n",
    "$$u^{[l]}=\\theta^{[l]}o^{[l-1]}+b^{[l]}$$\n",
    "where $u^{[l]} \\in R^{m}$ is a m-dimensional vector pertaining to the hidden units of the $l^{th}$ layer of the neural network after applying linear operations. Similarly, $o^{[l-1]} \\in R^{n}$ is the n-dimensional output vector corresponding to the hidden units of the $(l-1)^{th}$ activation layer. $\\theta^{[l]} \\in R^{m \\times n}$ is the weight matrix of the $l^{th}$ layer where each row of $\\theta^{[l]}$ is analogous to $\\theta_{i}$ described in the previous section i.e. each row corresponds to one hidden unit of the $l^{th}$ layer. $b^{[l]} \\in R^{m}$ is the bias vector of the layer where each element of b pertains to one hidden unit of the $l^{th}$ layer. This is followed by element wise non-linear activation function $o^{[l]} = \\phi(u^{[l]})$.\n",
    "The whole operation can be summarized as,\n",
    "$$o^{[l]} = \\phi(\\theta^{[l]}o^{[l-1]}+b^{[l]}) $$\n",
    "where $o^{[l-1]}$ is the output of the previous layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "A dropout layer is a regularization technique used in neural networks to reduce overfitting. During training, a dropout layer looks at each input unit and randomly decide if it will be dropped (set to zero) with some given probability $p$. The decision for each unit is made independently. Formally, given an input of shape $N \\times K$ (where $N$ is the number of data points and $K$ is the number of features), it samples from $\\text{Bernoulli}(p)$ for each unit, resulting in an output where approximately $pNK$ of the units are zero (in expectation). This forces the network to learn more robust and generalizable features, since it cannot rely too much on any particular input. During inference, the dropout layer is turned off, and the full network is used to make predictions. \n",
    "\n",
    "The dropout probability $p$ is a hyperparameter than can be tuned to adjust the strength of regularization. Setting $p=0$ is equivalent to no dropout. \n",
    "\n",
    "Note that the derivative of $\\text{dropout}(u)$ with respect to $u$ has the same shape as $u$. The values of the derivative depend on the random mask.\n",
    "\n",
    "Use [this](https://d2l.ai/chapter_multilayer-perceptrons/dropout.html) as a reference for your implementation.\n",
    "\n",
    "Note that after applying the mask, we must scale the result by a factor of $1/(1-p)$. Why is this necessary?\n",
    "\n",
    "<strong>TODO:</strong> Implement the <strong>_dropout</strong> function in <strong>NN.py</strong>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.localtests import TestNN\n",
    "TestNN('test_dropout').test_dropout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy Loss\n",
    "\n",
    "Cross-Entropy Loss is a widely used loss function in machine learning and deep learning, especially for classification tasks. It measures the dissimilarity between the predicted probability distribution and the true probability distribution of a classification problem. If it is closer to zero, the better the learnt function is.\n",
    "\n",
    "### Implementation details\n",
    "For classification problems as in this exercise, we compute the loss as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "CE = -\\frac{1}{N}\\sum\\limits_{i=1}^{N}\\left(y_{i} \\cdot log(\\hat{y_{i}})\\right)\n",
    "\\end{align*}\n",
    "\n",
    "where $y_{i}$ is the true label and $\\hat{y_{i}}$ is the estimated label. \n",
    "\n",
    "<strong>TODO:</strong> Implement the <strong>cross_entropy_loss</strong> function in <strong>NN.py</strong>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.localtests import TestNN\n",
    "\n",
    "TestNN('test_loss').test_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture\n",
    "\n",
    "*The architecture of our neural network.*\n",
    "\n",
    "![Neural Network](data/images/nn_architecture.png)\n",
    "\n",
    "\n",
    "The above diagram shows the dimensions of the neural network you will implement, along with the relationships between the quantities. Note that the neural network consists of two linear layers, with a leaky ReLU activation in between. The logits outputted by the second linear layer are passed through the softmax function, which turns them into probability distributions over the 3 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "We start by initializing the weights of the fully connected layer using [Xavier initialization](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf). (At a high level, we are using a uniform distribution for weight initialization). This is already implemented for you.\n",
    "\n",
    "## Forward Propagation\n",
    "During training, we pass all data points through the network, layer by layer, using forward propagation. The equations for forward propagation are as follows:\n",
    "\\begin{align*}\n",
    "u^{[0]} &= x\\\\\n",
    "u^{[1]}&= \\theta^{[1]}u^{[0]}+b^{[1]} \\\\\n",
    "o^{[1]}&= \\text{Dropout}(\\text{LeakyRelu}(u^{[1]})) \\\\\n",
    "u^{[2]}&= \\theta^{[2]}o^{[1]}+b^{[2]} \\\\\n",
    "\\hat{y}=o^{[2]}&= \\text{Softmax}(u^{[2]}).\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "We then use the output of the network to compute the loss\n",
    "\\begin{align*}\n",
    "CE = -\\frac{1}{N}\\sum\\limits_{i=1}^{N}\\left(y_{i} \\cdot log(\\hat{y_{i}})\\right)\n",
    "\\end{align*}\n",
    "\n",
    "<strong>TODO:</strong> Implement the <strong>forward</strong> function in <strong>NN.py</strong>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.localtests import TestNN\n",
    "\n",
    "TestNN('test_forward_without_dropout').test_forward_without_dropout()\n",
    "TestNN('test_forward').test_forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Propagation: Update Weights and Compute Gradients\n",
    "After the forward pass, we do back propagation to update the weights and biases in the direction of the negative gradient of the loss function. \n",
    "\n",
    "### Update Weights\n",
    "So, we update the weights and biases using the following formulas\n",
    "\\begin{align*}\n",
    "\\theta^{[2]} := \\theta^{[2]} - lr \\times \\frac{\\partial l}{\\partial \\theta^{[2]}} \\\\\n",
    "b^{[2]} := b^{[2]} - lr \\times \\frac{\\partial l}{\\partial b^{[2]}} \\\\\n",
    "\\theta^{[1]} := \\theta^{[1]} - lr \\times \\frac{\\partial l}{\\partial \\theta^{[1]}} \\\\\n",
    "b^{[1]} := b^{[1]} - lr \\times \\frac{\\partial l}{\\partial b^{[1]}}\n",
    "\\end{align*}\n",
    "where $lr$ is the learning rate. It decides the step size we want to take in the direction of the negative gradient. \n",
    "\n",
    "<strong>TODO:</strong> Implement the <strong>update_weights</strong> function in <strong>NN.py</strong> with use_momentum=False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.localtests import TestNN\n",
    "\n",
    "TestNN('test_update_weights').test_update_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Weights with Momentum [Bonus for Undergrad]\n",
    "\n",
    "Gradient descent does a generally good job of facilitating the convergence of the model's parameters to minimize the loss function. However, the process of doing so can be slow and/or noisy. **Momentum** is a technique used to stabilize this convergence. \n",
    "\n",
    "As a reminder, vanilla gradient descent applies the following update function to the parameters:\n",
    "$$ \n",
    "\\begin{equation}\n",
    "\\theta_{t+1} = \\theta_t - \\alpha \\nabla f(\\theta_t)\n",
    "\\end{equation}\n",
    "$$\n",
    "where $\\theta_t$ represents the parameters at time $t$, $\\alpha$ represents the learning rate, and $f$ is the loss function.\n",
    "\n",
    "Momentum proposes the following tweak to our parameter update function:\n",
    "$$\n",
    "\\begin{align*}\n",
    "z_{t+1} &= \\beta z_t + \\nabla f(\\theta_t) \\\\\n",
    "\\theta_{t+1} &= \\theta_t - \\alpha z_{t+1}\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\beta \\in [0, 1]$ is the momentum constant and $z_t$ represents the momentum records at time $t$.\n",
    "\n",
    "You can think of momentum as taking our previous changes into consideration. If we've been moving in a certain direction recently, it's likely we should keep moving in that direction. The recurrence relation given shows that we use an exponentially-weighted average of the previous updates for our current update.\n",
    "\n",
    "A useful analogy about momentum from [this great article on Distill](https://distill.pub/2017/momentum/):\n",
    "> Hereâ€™s a popular story about momentum: gradient descent is a man walking down a hill. He follows the steepest path downwards; his progress is slow, but steady. Momentum is a heavy ball rolling down the same hill. The added inertia acts both as a smoother and an accelerator, dampening oscillations and causing us to barrel through narrow valleys, small humps and local minima.\n",
    "\n",
    "<strong>TODO:</strong> Implement the <strong>update_weights</strong> function in <strong>NN.py</strong> with use_momentum=True.\n",
    "\n",
    "**HINT**: $z$ is stored in `self.ch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.localtests import TestNN\n",
    "\n",
    "TestNN('test_update_weights_with_momentum').test_update_weights_with_momentum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compute the gradients of the loss with respect to each parameter, we use the equations that make up the forward pass:\n",
    "\\begin{align*}\n",
    "u_1 &= \\theta_1 X + b_1 \\\\\n",
    "o_1 &= \\text{leaky\\_relu}(u_1) \\\\\n",
    "u_2 &= \\theta_2 o_1 + b_2 \\\\\n",
    "o_2 &= \\text{softmax}(u_2) \\\\\n",
    "l &= \\text{cross\\_entropy}(o_2)\n",
    "\\end{align*}\n",
    "\n",
    "When computing gradients, we travel backwards from the loss all the way back ot the input. We first seek to obtain the derivative of the loss $l$ with respect to the logits $u_2$. Note that they have the relation: $$ l = \\text{cross\\_entropy}(\\text{softmax}(u_2))$$ Computing the derivative of this seems very involved, but it actually has a very elegant result: $$ \\frac{\\partial l}{\\partial u_2} = \\text{softmax}(u_2) - y = \\hat{y} - y. $$ While this is given to you, we encourage you to derive it for yourself! You can find a great explanation of the derivation [in this article](https://towardsdatascience.com/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1).\n",
    "\n",
    "Now that we have $\\frac{\\partial l}{\\partial u_2}$, we seek to move further back and compute $\\frac{\\partial l}{\\partial \\theta_2}$ and $\\frac{\\partial l}{\\partial b_2}$. This is done using the chain rule:\n",
    "\\begin{align*}\n",
    "\\frac{\\partial l}{\\partial \\theta_2} &= \\frac{\\partial l}{\\partial u_2} \\cdot \\frac{\\partial u_2}{\\partial \\theta_2} \\\\\n",
    "\\frac{\\partial l}{\\partial b_2} &= \\frac{\\partial l}{\\partial u_2} \\cdot \\frac{\\partial u_2}{\\partial b_2}.\n",
    "\\end{align*}\n",
    "\n",
    "The quantities $\\frac{\\partial u_2}{\\partial \\theta_2}$ and $\\frac{\\partial u_2}{\\partial b_2}$ are easy to derive from the relation $u_2 = \\theta_2 o_1 + b_2$. We see that\n",
    "\\begin{align*}\n",
    "\\frac{\\partial l}{\\partial \\theta_2} &= \\frac{\\partial l}{\\partial u_2} \\cdot o_1 \\\\\n",
    "\\frac{\\partial l}{\\partial b_2} &= \\frac{\\partial l}{\\partial u_2} \\cdot 1.\n",
    "\\end{align*}\n",
    "\n",
    "Note that the derivative involves $o_1$, which we computed during the forward pass. Fortunately, we saved that value in `self.cache`, so we don't need to compute it again!\n",
    "\n",
    "The same procedure is repeated to obtain the gradients for the upstream parameters $\\theta_1$ and $b_1$. We must first perform the intermediate steps of computing the derivative of the loss with respect to $o_1$ and then $u_1$. These are given by \n",
    "\\begin{align*}\n",
    "\\frac{\\partial l}{\\partial o_1} &= \\frac{\\partial l}{\\partial u_2} \\cdot \\theta_2 \\\\\n",
    "\\frac{\\partial l}{\\partial u_1} &= \\frac{\\partial l}{\\partial o_1} \\cdot \\frac{\\partial\\,\\text{leaky\\_relu}}{\\partial u_1}.\n",
    "\\end{align*}\n",
    "\n",
    "In the second relation, we must consider our use of dropout! If we applied dropout on a particular neuron, it should not be adjusted. To account for this, in the case of `use_dropout=True`, we must instead use $$ \\frac{\\partial l}{\\partial u_1} = \\frac{\\partial l}{\\partial o_1} \\cdot \\frac{\\partial\\,\\text{leaky\\_relu}}{\\partial u_1} \\cdot \\text{dropout\\_mask} \\cdot \\frac{1}{1-p}, $$ where $1 / (1-p)$ is the scaling factor and dropout_mask is stored in `self.cache`.\n",
    "\n",
    "The final step! We can use these values to compute the gradients for $\\theta_1$ and $b_1$, using the relation $u_1 = \\theta_1 X + b_1$, which are given by\n",
    "\\begin{align*}\n",
    "\\frac{\\partial l}{\\partial \\theta_1} &= \\frac{\\partial l}{\\partial u_1} \\cdot X \\\\\n",
    "\\frac{\\partial l}{\\partial b_1} &= \\frac{\\partial l}{\\partial u_1} \\cdot 1.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation Tips\n",
    "The above equations are given in matrix notation. When implementing these computations in code, the easiest way to make sure you are calculating the values correctly and in the right order is to check shapes. Any time you are doing a matrix/vector operation in NumPy, **check the shapes**.\n",
    "\n",
    "Since we are computing these gradients over $N$ data points, we must divide the gradients by $N$ to take the _average_ gradient. Make sure you are dividing by $N$ exactly once, no more and no less!\n",
    "\n",
    "<strong>TODO:</strong> Implement the <strong>compute_gradients</strong> function in <strong>NN.py</strong>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.localtests import TestNN\n",
    "\n",
    "TestNN('test_compute_gradients_without_dropout').test_compute_gradients_without_dropout()\n",
    "TestNN('test_compute_gradients').test_compute_gradients()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to compute relevant gradients and how to update the weights of our network, we can perform the entire backwards step.\n",
    "\n",
    "<strong>TODO:</strong> Implement the <strong>backward</strong> function in <strong>NN.py</strong>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Local Test: Gradient Descent\n",
    "You may test your implementation of the GD function contained in **NN.py** in the cell below. See [Using the Local Tests](#using_local_tests) for more details. Look at the function documentation in gradient_descent for guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "from utilities.localtests import TestNN\n",
    "TestNN('test_gradient_descent').test_gradient_descent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4B3KRmadPzh"
   },
   "source": [
    "### 1.1.2 Local Test: Batch Gradient Descent [No Points]\n",
    "You may test your implementation of the BGD function contained in **NN.py** in the cell below. See [Using the Local Tests](#using_local_tests) for more details. Look at the function documentation in gradient_descent for guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A6NPnjiDdPzi",
    "outputId": "f70e8ae7-54e1-450b-ceae-ad914a708b14"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from utilities.localtests import TestNN\n",
    "TestNN('test_batch_gradient_descent').test_batch_gradient_descent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Local Test: Gradient Descent with Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may test your implementation of the GD function with momentum contained in **NN.py** in the cell below. See [Using the Local Tests](#using_local_tests) for more details. Revisit your implementation for update_weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from utilities.localtests import TestNN\n",
    "TestNN(\"test_gradient_descent_with_momentum\").test_gradient_descent_with_momentum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B79i8H2TMnPA"
   },
   "source": [
    "## 1.2 Loss plot and CE value for NN with Gradient Descent [5pts] <span style=\"color:green\">**[W]**</span>\n",
    "Train your neural net implementation with gradient descent and print out the loss at every 1000th iteration (starting at iteration 0). The following cells will plot the loss vs epoch graph and calculate the final test CE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4nJudgS-MnPB"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "from NN import NeuralNet\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "x_train, y_train, x_test, y_test = get_housing_dataset()\n",
    "\n",
    "nn = NeuralNet(y_train,lr=0.01,use_dropout=False, use_momentum=False) # initalize neural net class\n",
    "nn.gradient_descent(x_train, y_train, iter = 60000) #train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "y_pred = nn.predict(x_test)\n",
    "display_labels = ['low', 'med', 'high']\n",
    "ConfusionMatrixDisplay.from_predictions(y_true, y_pred, normalize='true', display_labels=display_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "fig = plt.plot(np.array(nn.loss).squeeze())\n",
    "plt.title(f'Training: {nn.neural_net_type}')\n",
    "plt.xlabel(\"Epoch (1000)\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OZLnsEBrMnPB",
    "outputId": "204445c4-5ae6-4a80-e900-5d451cc3c10d"
   },
   "outputs": [],
   "source": [
    "# Total loss\n",
    "y_hat = nn.forward(x_test, use_dropout=False)\n",
    "print(\"Cross entropy loss:\", round(nn.cross_entropy_loss(y_test, y_hat), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_4udQLmkMnPC"
   },
   "source": [
    "## 1.3 Loss plot and CE value for NN with BGD [5pts Bonus for Undergrad] <span style=\"color:green\">**[W]**</span>\n",
    "Train your neural net implementation with batch gradient descent and print out the loss at every 1000th iteration (starting at iteration 0). The following cells will plot the loss vs epoch graph and calculate the final test CE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OVTSILdPMnPC",
    "outputId": "6416c1dc-0aad-4f65-f959-410ba0be5d45"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "from NN import NeuralNet\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "x_train, y_train, x_test, y_test = get_housing_dataset()\n",
    "\n",
    "nn = NeuralNet(y_train,lr=.01,use_dropout=True, use_momentum=False) # initalize neural net class\n",
    "nn.batch_gradient_descent(x_train, y_train, iter = 60000, use_momentum=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "fig = plt.plot(np.array(nn.loss).squeeze())\n",
    "plt.title(f'Training: {nn.neural_net_type}')\n",
    "plt.xlabel(\"Epoch (1000)\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "y_pred = nn.predict(x_test)\n",
    "display_labels = ['low', 'med', 'high']\n",
    "ConfusionMatrixDisplay.from_predictions(y_true, y_pred, normalize='true', display_labels=display_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ERh21q9MnPD",
    "outputId": "35d527c7-4f63-4c35-bd6b-c510c85aef0a"
   },
   "outputs": [],
   "source": [
    "# Total loss\n",
    "y_hat = nn.forward(x_test, use_dropout=False)\n",
    "print(\"Cross entropy loss:\", round(nn.cross_entropy_loss(y_test, y_hat), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Loss plot and CE value for NN with Gradient Descent with Momentum [5pts Bonus for Undergrad] <span style=\"color:green\">**[W]**</span>\n",
    "Train your neural net implementation with gradient descent with momentum and print out the loss at every 1000th iteration (starting at iteration 0). The following cells will plot the loss vs epoch graph and calculate the final test CE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "from NN import NeuralNet\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "x_train, y_train, x_test, y_test = get_housing_dataset()\n",
    "\n",
    "nn = NeuralNet(y_train,lr=1,use_dropout=False, use_momentum=True) # initalize neural net class\n",
    "nn.gradient_descent(x_train, y_train, iter = 60000, use_momentum=True) #train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "fig = plt.plot(np.array(nn.loss).squeeze())\n",
    "plt.title(f'Training: {nn.neural_net_type}')\n",
    "plt.xlabel(\"Epoch (1000)\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "y_pred = nn.predict(x_test)\n",
    "display_labels = ['low', 'med', 'high']\n",
    "ConfusionMatrixDisplay.from_predictions(y_true, y_pred, normalize='true', display_labels=display_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total loss\n",
    "y_hat = nn.forward(x_test, use_dropout=False)\n",
    "print(\"Cross entropy loss:\", round(nn.cross_entropy_loss(y_test, y_hat), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ayv_tqnyxMb0"
   },
   "source": [
    "# 2: Image Classification based on Convolutional Neural Networks [25pts; 20pts Bonus for Undergrad + 5pts Bonus for all] <span style=\"color:blue\">**[P]**</span><span style=\"color:green\">**[W]**</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NAcYa40pm86P"
   },
   "source": [
    "## 2.1 Image Classification using Pytorch and CNN\n",
    "\n",
    "- [Pytorch](https://pytorch.org) is a popular platform for machine learning.\n",
    "\n",
    "**Pytorch Description**\n",
    "\n",
    "PyTorch is a Machine Learning/Deep Learning tensor library based on Python and Torch. It uses dynamic computation graphs and is completely Pythonic. Pytorch is used for applications using GPUs and CPUs. \n",
    "\n",
    "**Helpful Links**\n",
    "\n",
    "- [Install Pytorch](https://pytorch.org/get-started/locally/)\n",
    "- [Pytorch Quickstart Tutorial](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup Pytorch**\n",
    "\n",
    "Make sure you installed pytorch and torchvision (directions [here](https://pytorch.org/get-started/locally/)).\n",
    "\n",
    "Please also see [Pytorch Quickstart Tutorial](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html) to see how to load a data set, build a training loop, and test the model. Another good resource for building CNNs using Pytorch is [here](https://pyimagesearch.com/2021/07/19/pytorch-training-your-first-convolutional-neural-network-cnn/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWSeZNmCm86P"
   },
   "source": [
    "### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X2Cwef24xgtT"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import non_deterministic\n",
    "import torchvision\n",
    "import torch\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D5wmr3mt2yOu"
   },
   "source": [
    "### 2.1.1 Load CIFAR-10 Dataset and Data Augmentation [5pts - Bonus for Undergrad]<span style=\"color:blue\">**[P]**</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PxDuP6Yq2yOv"
   },
   "source": [
    "We use [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset to train our model. This is a dataset of 60,000 32x32 colour images in 10 classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images. We provide code for you to download CIFAR-10 dataset below.  \n",
    "\n",
    "#### Data Augmentation [5pts]\n",
    "\n",
    "Data augmentation is a technique to increase the diversity of your training set by applying random (but realistic) transformations such as image rotation and flipping the image around an axis. If the dataset in a machine learning model is rich and sufficient, the model performs better and more accurately. We will preprocess the training and testing set, but only the training set will undergo augmentation.\n",
    "\n",
    "Go through the [Pytorch torchvision.transforms.v2 documentation](https://pytorch.org/vision/master/transforms.html) to see how to apply multiple transformations at once.\n",
    "\n",
    "In the <strong>cnn_image_transformations.py</strong> file, complete the following functions to understand the common practices used for preprocessing and augmenting the image data:\n",
    "  * <strong>create_training_transformations</strong>\n",
    "    * In this function, you are going to preprocess and augment training data.\n",
    "      \n",
    "      * PREPROCESS: Convert the given PIL Images to Tensors\n",
    "\n",
    "      * AUGMENTATION: Apply Random Horizontal Flip and Random Rotation\n",
    "\n",
    "  * <strong>create_testing_transformations</strong>\n",
    "    * In this function, you are going to only preprocess testing data.\n",
    "      \n",
    "      * PREPROCESS: Convert the given PIL Images to Tensors\n",
    "\n",
    "  \n",
    "Please note that the Gradescope only checks if expected preprocessing layers are existent.\n",
    "\n",
    "**References**\n",
    "\n",
    "[v2.Compose()](https://pytorch.org/vision/main/generated/torchvision.transforms.v2.Compose.html)\n",
    "\n",
    "[v2.ToTensor()](https://pytorch.org/vision/main/generated/torchvision.transforms.v2.ToTensor.html) (Hint: Look at the warning)\n",
    "\n",
    "[v2.RandomHorizontalFlip()](https://pytorch.org/vision/main/generated/torchvision.transforms.RandomHorizontalFlip.html)\n",
    "\n",
    "[v2.RandomApply()](https://pytorch.org/vision/main/generated/torchvision.transforms.v2.RandomApply.html)\n",
    "\n",
    "[v2.RandomRotation()](https://pytorch.org/vision/main/generated/torchvision.transforms.RandomRotation.html)\n",
    "\n",
    "[Article about performance regarding transformations](https://pytorch.org/vision/master/transforms.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zv8fRYkp2yOv",
    "outputId": "b943ca12-7c2f-470b-fc54-d03c4d25910d"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from cnn_image_transformations import create_training_transformations\n",
    "from cnn_image_transformations import create_testing_transformations\n",
    "\n",
    "# Create Transformations\n",
    "training_transformations = create_training_transformations()\n",
    "testing_transformation = create_testing_transformations()\n",
    "\n",
    "# Load data\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=training_transformations)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=testing_transformation)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "print(trainset.data.shape)\n",
    "print(testset.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IropaXmi2yOy"
   },
   "source": [
    "### 2.1.2 Load some sample images from CIFAR-10 [Setup - No points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 593
    },
    "id": "NaUQz7x52yOy",
    "outputId": "efcf12cc-5cc1-4c46-e779-d9b15165a678"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "print(\"Image size\")\n",
    "print(v2.functional.get_size(images[0]))\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJEjYyism86Y"
   },
   "source": [
    "As you can see from above, the CIFAR-10 dataset contains different types of objects. The images have been size-normalized and objects remain centered in fixed-size images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deWtvYFKm86Z"
   },
   "source": [
    "### 2.1.3 Build convolutional neural network model [5pts] <span style=\"color:green\">**[W]**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8ioOgl1m86Z"
   },
   "source": [
    "In this part, you need to build a convolutional neural network as described below. The architecture of the model is outlined.\n",
    "\n",
    "In the <strong>cnn.py</strong> file, complete the following functions:\n",
    "  * <strong> \\__init\\__</strong>: See Defining Variables section\n",
    "  * <strong>forward</strong>: See Defining Model section\n",
    "\n",
    "\n",
    "\n",
    " **[INPUT - CONV - CONV - MAXPOOL - DROPOUT - CONV - CONV - MAXPOOL - DROPOUT - AVERAGEPOOL - FC1 - DROPOUT - FC2 - DROPOUT - FC3]**\n",
    "\n",
    "> INPUT: [$32\\times32\\times3$] will hold the raw pixel values of the image, in this case, an image of width 32, height 32, with 3 channels.\n",
    "\n",
    "> CONV: Conv. layer will compute the output of neurons that are connected to local regions in the input, each computing a dot product between their weights and a small region they are connected to the input volume. In our example architecture, we decide to set the kernel_size to be $3\\times3$. For example, the output of the Conv. layer may look like $[32\\times32\\times8]$ if we set out_channels to be 8 and use appropriate paddings to maintain shape.\n",
    "\n",
    "> CONV: Additional Conv. layer take outputs from above layers and applies more filters. We set the kernel_size to be $3\\times3$ and out_channels to be 32.\n",
    "\n",
    "> MAXPOOL: MAXPOOL layer will perform a downsampling operation along the spatial dimensions (width, height). With pool size of $2\\times2$, resulting shape takes form $16\\times16$.\n",
    "\n",
    "> DROPOUT: DROPOUT layer with the dropout rate of 0.2 to prevent overfitting.\n",
    "\n",
    "> CONV: Additonal Conv. layer takes outputs from above layers and applies more filters. We set the kernel_size to be $3\\times3$ and out_channels to be 32. Appropriate paddings are used to maintain shape.\n",
    "\n",
    "> CONV: Additonal Conv. layer takes outputs from above layers and applies more filters. We set the kernel_size to be $3\\times3$ and out_channels to be 64. Appropriate paddings are used to maintain shape.\n",
    "\n",
    "> MAXPOOL: MAXPOOL layer will perform a downsampling operation along the spatial dimensions (width, height).\n",
    "\n",
    "> DROPOUT: Dropout layer with the dropout rate of 0.2 to prevent overfitting.\n",
    "\n",
    "> AVERAGEPOOL: AVERAGEPOOL layer will perform a downsampling operation along the spatial dimension (width, height). Checkout AdaptiveAvgPool2d below.\n",
    "\n",
    "> FC1: Dense layer which takes output from above layers, and has 256 neurons. Flatten() operations may be useful.\n",
    "\n",
    "> DROPOUT: Dropout layer with the dropout rate of 0.2 to prevent overfitting.\n",
    "\n",
    "> FC2ï¼š Dense layer which takes output from above layers, and has 128 neurons. \n",
    "\n",
    "> DROPOUT: Dropout layer with the dropout rate of 0.2 to prevent overfitting.\n",
    "\n",
    "> FC3: Dense layer with 10 neurons, and Softmax activation, is the final layer. The dimension of the output space is the number of classes.\n",
    "\n",
    "**Activation function**: Use LeakyReLU with negative_slope 0.01 as the activation function for Conv. layers and Dense layers unless otherwise indicated to build you model architecture\n",
    "\n",
    "Note that while this is a suggested model design, you are welcome to use other architectures and experiment with different layers for better results. \n",
    "\n",
    "The following links are Pytorch documentation for the layers you are going to use to build the CNN.\n",
    "\n",
    "- [Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)\n",
    "- [Dense](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n",
    "- [MaxPool](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html)\n",
    "- [AdaptiveAvgPool2d](https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html)\n",
    "- [Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html)\n",
    "- [LeakyReLU](https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html)\n",
    "- [Flatten](https://pytorch.org/docs/stable/generated/torch.flatten.html)\n",
    "\n",
    "Lastly, if you would like to experiment with additional layers, explore the [torch.nn api](https://pytorch.org/docs/stable/nn.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "id": "L6fLE8Wpm86Z",
    "outputId": "ccbd55e9-aba5-4337-9b72-028085f0967b"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "# Show the architecture of the model\n",
    "achi=plt.imread('./data/images/Architecture.png')\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.imshow(achi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5F5PWx17m86g"
   },
   "source": [
    "#### Defining model [5pts Bonus for Undergrad]<span style=\"color:green\">**[W]**</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s509lYZCMnPJ"
   },
   "source": [
    "You now need to complete the `__init__()` function and the `forward()` function in <strong>cnn.py</strong> to define your model structure.\n",
    "\n",
    "Your model is required to have at least 2 convolutional layers and at least 2 dense layers. Ensuring that these requirements are met will earn you 5pts.\n",
    "\n",
    "Once you have defined a model structure you may use the cell below to examine your architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tKpTMi2rm86i",
    "outputId": "2014de42-de96-48da-df1e-697ea5d7b634",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "# You can compare your architecture with the 'Architecture.png'\n",
    "\n",
    "from cnn import CNN\n",
    "net = CNN()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcCxKieHRk6L"
   },
   "source": [
    "### 2.1.4 Train the network [8pts total (3pts, 3pts, 2pts) Bonus for Undergrad] <span style=\"color:green\">**[W]**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSPkbMIypNJ6"
   },
   "source": [
    "**Tuning:** Training the network is the next thing to try.  You can set the hyperparameters in the cell below. If your hyperparameters are set properly, you should see the loss of the validation set decreased and the value of accuracy increased. <strong>It may take more than 15 minutes to train your model. </strong>\n",
    "\n",
    "- Recommended Batch Sizes fall in the range 32-512 (use powers of 2)\n",
    "\n",
    "- Recommended Epoch Counts fall in the range 5-20\n",
    "\n",
    "- Recommended Learning Rates fall in the range .0001-.01\n",
    "\n",
    "**Expected Result:** You should be able to achieve more than $79\\%$ accuracy on the test set to get full points. If you achieve accuracy between $60\\%$ to $69\\%$, you will only get 3 points. An accuracy between $69\\%$ to $75\\%$ will earn an additional 3pts.\n",
    "\n",
    "Note: If you would like to automate the tuning process, you can use a nested for loop to search for the hyperparameter that achieves the accuracy.\n",
    "\n",
    "- $60\\%$ to $69\\%$ earns 3pts\n",
    "- $69\\%$ to $75\\%$ earns 3pts more (6pts total)\n",
    "- $75\\%$+ earns 2pts more (8pts total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXExabcOpAdI"
   },
   "source": [
    "#### Train your own CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eeNII5h0EhBa",
    "outputId": "20c25acc-9f2e-4b86-9924-03d2eb545141"
   },
   "outputs": [],
   "source": [
    "from cnn import CNN\n",
    "from cnn_trainer import Trainer\n",
    "\n",
    "net = CNN()\n",
    "\n",
    "# TODO: Change hyperparameters here\n",
    "num_epochs = 3\n",
    "batch_size = 16\n",
    "init_lr = 1e-3\n",
    "\n",
    "# Choose best device to speed up training\n",
    "device = (\"cuda\" if torch.cuda.is_available() \n",
    "          else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "trainer = Trainer(net, trainset, testset, num_epochs=num_epochs, batch_size=batch_size, init_lr=init_lr, device=device)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xv_HZge7MnPL"
   },
   "source": [
    "### 2.1.5 Examine accuracy and loss [2pts Bonus for Undergrad] <span style=\"color:green\">**[W]**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OkcOlCqDMnPL"
   },
   "source": [
    "You should expect to see gradually decreasing loss and gradually increasing accuracy. Examine loss and accuracy by running the cell below, no editing is necessary. Having appropriate looking loss and accuracy plots will earn you the last 2pts for your convolutional neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 590
    },
    "id": "w0dD-7Edm86o",
    "outputId": "85b9f1ed-fe71-4ad1-bd9d-22100dcdc9b7"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "# list all data in history\n",
    "train_loss, train_accuracy, valid_loss, valid_accuracy = trainer.get_training_history()\n",
    "\n",
    "# summarize history for accuracy and loss\n",
    "plt.plot(train_accuracy)\n",
    "plt.plot(valid_accuracy)\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_loss)\n",
    "plt.plot(valid_loss)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 481
    },
    "id": "67Sc8YnEMnPM",
    "outputId": "9ddba3c0-4334-427a-e1d1-e20a1ad70c20"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "# make predictions\n",
    "y_pred, y_pred_classes,y_gt_classes = trainer.predict(testloader)\n",
    "y_pred_prob = torch.max(y_pred, dim=1).values\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "plt.figure(figsize=(8, 7))\n",
    "plt.imshow(confusion_matrix(y_gt_classes, y_pred_classes, normalize='true'))\n",
    "plt.title('Confusion matrix', fontsize=16)\n",
    "plt.xticks(np.arange(10), classes, rotation=90, fontsize=12)\n",
    "plt.yticks(np.arange(10), classes, fontsize=12)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mkn21kyUObtK"
   },
   "source": [
    "## 2.2 Exploring Deep CNN Architectures [5pts Bonus for All] <span style=\"color:green\">**[W]**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhDboDvPObtK"
   },
   "source": [
    "The network you have produced is rather simple relative to many of those used in industry and research. Researchers have worked to make CNN models deeper and deeper over the past years in an effort to gain higher accuracy in predictions. While your model is only a handful of layers deep, some state of the art deep architectures may include up to 150 layers. However, this process has not been without challenges. \n",
    "\n",
    "One such problem is the problem of the vanishing gradient. The weights of a neural network are updated using the backpropagation algorithm. The backpropagation algorithm makes a small change to each weight in such a way that the loss of the model decreases. Using the chain rule, we can find this gradient for each weight. But, as this gradient keeps flowing backwards to the initial layers, this value keeps getting multiplied by each local gradient. Hence, the gradient becomes smaller and smaller, making the updates to the initial layers very small, increasing the training time considerably.\n",
    "\n",
    "Many tactics have been used in an effort to solve this problem. One architecture, named ResNet, solves the vanishing gradient problem in a unique way. ResNet was developed at Microsoft Research to find better ways to train deep networks. Take a moment to explore how ResNet tackles the vanishing gradient problem by reading the original research paper here: https://arxiv.org/pdf/1512.03385.pdf (also included as PDF in papers directory). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7JcSZtyObtK"
   },
   "source": [
    "**Question:** In your own words, explain how ResNet addresses the vanishing gradient problem in 1-2 sentences below: (Please type answers directly in the cell below.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IU_9UwLTObtK"
   },
   "source": [
    "**Answer:** \n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6FxKE3QN2yO6"
   },
   "source": [
    "# 3: Random Forests [45pts; 40pts + 5pts Bonus for All] <span style=\"color:blue\">**[P]**</span> <span style=\"color:green\">**[W]**</span>\n",
    "\n",
    "**NOTE**:  Please use sklearn's ExtraTreeClassifier in your Random Forest implementation. [You can find more details about this classifier here.](https://scikit-learn.org/stable/modules/generated/sklearn.tree.ExtraTreeClassifier.html)\n",
    "\n",
    "For context, the general difference between an extra tree and decision tree classifier is that the decision tree optimizes which feature to reduce entropy on and at what value to split, while an extra tree randomly splits on the features given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5zP6DnH62yO9"
   },
   "source": [
    "## 3.1 Random Forest Implementation [35pts] <span style=\"color:blue\">**[P]**</span>\n",
    "\n",
    "The decision boundaries drawn by decision or extra trees are very sharp, and fitting a tree of unbounded depth to a list of examples almost inevitably leads to **overfitting**. In an attempt to decrease the variance of an extra tree, we're going to use a technique called 'Bootstrap Aggregating' (often abbreviated 'bagging'). This stems from the idea that a collection of weak learners can learn decision boundaries as well as a strong learner. This is commonly called a Random Forest.\n",
    "\n",
    "We can build a Random Forest as a collection of extra trees, as follows:\n",
    "\n",
    "1. For every tree in the random forest, we're going to \n",
    "\n",
    "    a) Subsample the examples with replacement. Note that in this question, the size of the subsample data is equal to the original dataset. \n",
    "    \n",
    "    b) From the subsamples in part a, choose attributes at random without replacement to learn on in accordance with a provided attribute subsampling rate. Based on what it was mentioned in the class, we randomly pick features in each split. We use a more general approach here to make the programming part easier. Let's randomly pick some features (65% percent of features) and grow the tree based on the pre-determined randomly selected features. Therefore, there is no need to find random features in each split.\n",
    "    \n",
    "    c) Fit an extra tree to the subsample of data we've chosen to a certain depth.\n",
    "\n",
    "    \n",
    "Classification for a random forest is then done by taking a majority vote of the classifications yielded by each tree in the forest after it classifies an example.\n",
    "\n",
    "In the <strong>random_forest.py</strong> file, complete the following functions:\n",
    "  * <strong>_bootstrapping</strong>: this function will be used in `bootstrapping()`\n",
    "  * <strong>fit</strong>: Fit the extra trees initialized in `__init__` with the datasets created in `bootstrapping()`. You will need to call `bootstrapping()`. \n",
    "\n",
    "**NOTES:**\n",
    "1. In the Random Forest Class, X is assumed to be a matrix with num_training rows and num_features columns where num_training is the number of total records and num_features is the number of features of each record. y is assumed to be a vector of labels of length num_training.\n",
    "2. Look out for TODO's for the parts that need to be implemented\n",
    "3. If you receive any ``SettingWithCopyWarning`` warnings from the Pandas library, you can safely ignore them.\n",
    "4.  Hint: when bootstrapping, set replace = False while creating col_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8E74hGIm86t"
   },
   "source": [
    "## 3.2 Hyperparameter Tuning with a Random Forest [5pts] <span style=\"color:blue\">**[P]**</span>\n",
    "\n",
    "In machine learning, hyperparameters are parameters that are set before the learning process begins. The max_depth, num_estimators, or max_features variables from 3.1 are examples of different hyperparameters for a random forest model. Let's first review the dataset in a bit more detail.\n",
    "\n",
    "#### Dataset Objective\n",
    "\n",
    "Imagine that we are a team of researchers working to track and document various information related to dry beans for a machine learning model that predicts what type of bean is represented. We know that there are multiple things to keep track of, such as the shapes and sizes that differentiate different types of beans. We will use the information we track and document in order to publish it for the general public.\n",
    "\n",
    "After much reflection within the research team, we come to the conclusion that we can use past observations on bean images to create a model.\n",
    "\n",
    "We will use our random forest algorithm from Q3.1 to predict the bean type.\n",
    "\n",
    "You can find more information on the dataset [here](https://archive.ics.uci.edu/dataset/602/dry+bean+dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The barbunya bean, also known as the cranberry bean, was first bred in Colombia.*\n",
    "\n",
    "![A barbunya bean](data/images/barbunya.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYwbegdZm86t"
   },
   "source": [
    "#### Loading the dataset\n",
    "\n",
    "\n",
    "The dataset that the company has collected has the following features:\n",
    "\n",
    "There were 16 features used in this dataset.\n",
    "\n",
    "Inputs:\n",
    "\n",
    "\n",
    "1. Area\tThe area of a bean zone and the number of pixels within its boundaries\t\n",
    "2. Perimeter: Bean circumference is defined as the length of its border\n",
    "3. MajorAxisLength: The distance between the ends of the longest line that can be drawn from a bean\n",
    "4. MinorAxisLength: The longest line that can be drawn from the bean while standing perpendicular to the main axis\n",
    "5. AspectRatio: Defines the relationship between MajorAxisLength and MinorAxisLength\t\n",
    "6. Eccentricity: Eccentricity of the ellipse having the same moments as the region\n",
    "7. ConvexArea: Number of pixels in the smallest convex polygon that can contain the area of a bean seed\n",
    "8. EquivDiameter Equivalent diameter, the diameter of a circle having the same area as a bean seed area\n",
    "9. Extent\tFeature: The ratio of the pixels in the bounding box to the bean area\n",
    "10. Solidity: Also known as convexity. The ratio of the pixels in the convex shell to those found in beans.\n",
    "11. Roundness: Calculated with the following formula: (4piA)/(P^2)\t\n",
    "12. Compactness: Measures the roundness of an object\t\n",
    "13. ShapeFactor1\n",
    "14. ShapeFactor2\n",
    "15. ShapeFactor3\n",
    "16. ShapeFactor4\n",
    "\n",
    "Output:\n",
    "\n",
    "17. Target value:\n",
    "    * Seker\n",
    "    * Barbunya\n",
    "    * Bombay\n",
    "    * Cali\n",
    "    * Dermosan\n",
    "    * Horoz  \n",
    "    * Sira\n",
    "    \n",
    "Your random forest model will try to predict this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fN4HGAsXMnPO"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dry_bean_dataset = \"./data/Dry_Bean_Dataset.csv\"\n",
    "df = pd.read_csv(dry_bean_dataset)\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder() \n",
    "\n",
    "X = df.drop(['Class'], axis=1)\n",
    "y = label_encoder.fit_transform(df['Class']) \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42)\n",
    "X_test = np.array(X_test)\n",
    "X_train, y_train, X_test, y_test = (\n",
    "    np.array(X_train),\n",
    "    np.array(y_train),\n",
    "    np.array(X_test),\n",
    "    np.array(y_test),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2uFNR6ikMnPO",
    "outputId": "ad4681bd-45f9-4b4a-ebab-ca1793c07822"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "assert X_train.shape == (9119, 16)\n",
    "assert y_train.shape == (9119,)\n",
    "assert X_test.shape == (4492, 16)\n",
    "assert y_test.shape == (4492,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BlxXUpNE2yPA"
   },
   "source": [
    "In the following codeblock, train your random forest model with different values for max_depth, n_estimators, or max_features and evaluate each model on the held-out test set. Try to choose a combination of hyperparameters that maximizes your prediction accuracy on the test set (aim for 85%+). \n",
    "\n",
    "In **random_forest.py**, once you are satisfied with your chosen parameters, update the following function:\n",
    "- **select_hyperparameters**: change the values for ```max_depth```, ```n_estimators```, and ```max_features```to your chosen values\n",
    "\n",
    "Submit this file to Gradescope. You must achieve at least a **85% accuracy** against the test set in Gradescope to receive full credit for this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "from utilities.localtests import TestRandomForest\n",
    "\n",
    "'''\n",
    "Once you have implemented Random forest, you can run this cell. If you implemented _bootStrapping correctly,\n",
    "then this cell should execute without any errors.\n",
    "'''\n",
    "TestRandomForest(\"test_bootstrapping\").test_bootstrapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "id": "6n8GGVU7tYGh",
    "outputId": "4a83b962-d917-4a53-9dc8-2681735d9396"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO: \n",
    "n_estimators defines how many Extra trees are fitted for the random forest.\n",
    "max_depth defines a stop condition when the tree reaches to a certain depth.\n",
    "max_features controls the percentage of features that are used to fit each extra tree.\n",
    "\n",
    "Tune these three parameters to achieve a better accuracy. n_estimators and max_depth must both\n",
    "be at least 3 in value for moderately reliable answers. While you can use the provided test set\n",
    "to evaluate your implementation, you will need to obtain 85% on the test set to receive full\n",
    "credit for this section.\n",
    "\"\"\"\n",
    "from random_forest import RandomForest\n",
    "from sklearn import preprocessing\n",
    "import sklearn.ensemble\n",
    "\n",
    "################# DO NOT CHANGE THIS RANDOM SEED ####################\n",
    "student_random_seed = 4641 + 7641\n",
    "#####################################################################\n",
    "\n",
    "################# CHANGE THESE VALUES ###############################\n",
    "n_estimators = 3 #Hint: Consider values between 3-15.\n",
    "max_depth = 3 # Hint: Consider values betweeen 3-15.\n",
    "max_features = 0.3 # Hint: Consider values betweeen 0.3-1.0.\n",
    "#####################################################################\n",
    "random_forest = RandomForest(n_estimators, max_depth, max_features, random_seed=student_random_seed)\n",
    "random_forest.fit(X_train, y_train)\n",
    "accuracy=random_forest.OOB_score(X_test, y_test)\n",
    "print(\"accuracy: %.4f\" % accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIQa-UIoMnPP"
   },
   "source": [
    "**DON'T FORGET**: Once you are satisfied with your chosen parameters, change the values for ```max_depth```, ```n_estimators```, and ```max_features``` in the ```select_hyperparameters()``` function of your RandomForest class in ```random_forest.py``` to your chosen values, and then submit this file to Gradescope. You must achieve at least a **85% accuracy** against the test set in Gradescope to receive full credit for this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a code block that plots a confusion matrix for the classifier's predictions on the test set. A few things to think about: What are some trends seen in the matrix? Why do they happen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "pred = random_forest.predict(X_test)\n",
    "labels = [\"Seker\", \"Barbunya\", \"Bombay\", \"Cali\", \"Horoz\", \"Sira\", \"Dermason\"]\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, pred, display_labels=labels, normalize='true', xticks_rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQ3Urx3Em86y"
   },
   "source": [
    "## 3.3 Plotting Feature Importance [5pts Bonus for All] <span style=\"color:green\">**[W]**</span>\n",
    "\n",
    "While building tree-based models, it's common to quantify how well splitting on a particular feature in an extra tree helps with predicting the target label in a dataset. Machine learning practicioners typically use \"Gini importance\", or the (normalized) total reduction in entropy brought by that feature to evaluate how important that feature is for predicting the target variable.\n",
    "\n",
    "Gini importance is typically calculated as the reduction in entropy from reaching a split in an extra tree weighted by the probability of reaching that split in the extra tree. Sklearn internally computes the probability for reaching a split by finding the total number of samples that reaches it during the training phase divided by the total number of samples in the dataset. This weighted value is our feature importance.\n",
    "\n",
    "Let's think about what this metric means with an example. A high probabiity of reaching a split on feature A in an extra tree trained on a dataset (many samples will reach this split for a decision) and a large reduction in entropy from splitting on feature A will result in a high feature importance value for feature A. This could mean feature A is a very important feature for predicting the probability of the target label. On the other hand, a low probability of reaching a split on feature B in an extra tree and a low reduction in entropy from splitting on feature B will result in a low feature importance value. This could mean feature B is not a very informative feature for predicting the target label. **Thus, the higher the feature importance value, the more important the feature is to predicting the target label.**\n",
    "\n",
    "Fortunately for us, fitting a sklearn.ExtraTreeClassifier to a dataset auomatically computes the Gini importance for every feature in the extra tree and stores these values in a **feature\\_importances\\_** variable. [Review the docs for more details on how to access this variable](https://scikit-learn.org/stable/modules/generated/sklearn.tree.ExtraTreeClassifier.html#sklearn.tree.ExtraTreeClassifier.feature_importances_)\n",
    "\n",
    "In the <strong>random_forest.py</strong> file, complete the following function:\n",
    "  * <strong>plot_feature_importance</strong>: Make sure to sort the bars in descending order and remove any features with feature importance of 0\n",
    "  \n",
    "In the cell below, call your implementation of `plot_feature_importance()` and display a bar plot that shows the feature importance values for at least one extra tree in your tuned random forest from Q3.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "irV3hL6mm86z",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO: Complete plot_feature_importance() in random_forest.py\n",
    "\n",
    "random_forest.plot_feature_importance(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V0pMnninMnPV"
   },
   "source": [
    "Note that there isn't one \"correct\" answer here. We simply want you to investigate how different features in your random forest contribute to predicting the target variable.\n",
    "\n",
    "Also note that: the number of features can be different if you change max_features value since it ends up changing the number of features considered in bootstrapped datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s1udwVq0PVFz"
   },
   "source": [
    "# 4: (Bonus for All) SVM [34 pts] <span style=\"color:green\">**[W]**</span> <span style=\"color:blue\">**[P]**</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FG6jrvc_m861"
   },
   "source": [
    "## 4.1 Fitting an SVM classifier by hand [24 pts] <span style=\"color:green\">**[W]**</span>\n",
    "\n",
    "Consider a dataset with the following points in 2-dimensional space:\n",
    "\n",
    "| $$x_1$$ | $$x_2$$ | $$y$$ |\n",
    "| --- | --- | --- |\n",
    "| -1 | -1 | -1 |\n",
    "| -1 | 1 | -1 |\n",
    "| 1 | -1 | -1 |\n",
    "| 2 | 2 | 1 |\n",
    "| 2 | 3 | 1 |\n",
    "| 1 | 3 | 1 |\n",
    "\n",
    "Here, $x_1$ and $x_2$ are features and $y$ is the label.\n",
    "\n",
    "The max margin classifier has the formulation,\n",
    "\n",
    "$$\\min{||\\mathbf{\\theta}||^2} $$\n",
    "\n",
    "$$s.t.\\ y_i(\\mathbf{x_i} \\mathbf{\\theta} + b) â‰¥ 1 \\ \\ \\ \\ \\forall \\ i$$\n",
    "\n",
    "**Hint:** $\\mathbf{x_i}$ are the suppport vectors. Margin is equal to $\\frac{1}{||\\mathbf{\\theta}||}$ and full margin is equal to $\\frac{2}{||\\mathbf{\\theta}||}$. You might find it useful to plot the points in a 2D plane. When calculating the $\\theta$ you don't need to consider the bias term.\n",
    "\n",
    "(1) Are the points linearly separable? Does adding the point $\\mathbf{x} = (2, 1)$, $y = 1$ change the separability? (2 pts)\n",
    "\n",
    "(2) According to the max-margin formulation, find the separating hyperplane. Do not consider the new point from part 1 in your calculations for this current question or subsequent parts. (You should give some kind of explanation or calculation on how you found the hyperplane, you may solve this question graphically.) (4 pts)\n",
    "\n",
    "(3) Find a vector parallel to the optimal vector $\\mathbf{\\theta}$. (Hint: Recall whether the optimal vector is parallel or perpendicular to the separating hyperplane.) (4 pts)\n",
    "\n",
    "(4) Calculate the value of the margin (single-sided) achieved by this $\\mathbf{\\theta}$? (4 pts)\n",
    "\n",
    "(5) Solve for $\\mathbf{\\theta}$, given that the margin is equal to $1/||\\mathbf{\\theta}||$. (4 pts)\n",
    "\n",
    "(6) If we remove one of the points from the original data the SVM solution might change. Find all such points which change the solution. (2 pts)\n",
    "\n",
    "(7) Consider the optimization formulation stated above. Why do we want to optimzie $||\\mathbf{\\theta}||^2$ instead of $||\\mathbf{\\theta}||$? (2 pts)\n",
    "\n",
    "(8) Plot the features $x_1$ and $x_2$, based on label $y$ (use different color for different label), ignoring the hypothetical point mentioned in part (1). Please also included the separating hyperplane in the plot (4 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Responses:\n",
    "\n",
    "(1)\n",
    "\n",
    "(2)\n",
    "\n",
    "(3)\n",
    "\n",
    "(4)\n",
    "\n",
    "(5)\n",
    "\n",
    "(6)\n",
    "\n",
    "(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (question 8): plot the points listed in the table\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NacpSq-Tm862"
   },
   "source": [
    "## 4.2 Feature Mapping [10 pts] <span style=\"color:blue\">**[P]**</span>\n",
    "\n",
    "Let's look at a dataset where the datapoint can't be classified with a good accuracy using a linear classifier. Run the below cell to generate the dataset.\n",
    "\n",
    "We will also see what happens when we try to fit a linear classifier to the dataset.\n",
    "\n",
    "there are some suggestion readings:\n",
    "\n",
    "https://see.stanford.edu/materials/aimlcs229/cs229-notes3.pdf \n",
    "\n",
    "https://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf \n",
    "\n",
    "https://www.sjsu.edu/faculty/guangliang.chen/Math251F18/lec6svm.pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "LmyYQFamm862",
    "outputId": "2ab03801-ca5f-4f98-ffdf-a1ba1aa38f39"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "# Generate dataset\n",
    "\n",
    "random_state = 1\n",
    "\n",
    "np.random.seed(0)\n",
    "theta = np.linspace(0, 2*np.pi, 1000)\n",
    "r = np.random.uniform(0.8, 1.2, 1000)\n",
    "X = np.column_stack([r * np.cos(theta), r * np.sin(theta)])\n",
    "y = np.logical_or(theta < np.pi, theta >= 2 * np.pi)\n",
    "X[y == 0, 0] += 1\n",
    "X[y == 0, 1] += 0.5\n",
    "\n",
    "R = np.array([[0, -1], \n",
    "              [1, 0]])\n",
    "\n",
    "X_rotated = X.dot(R.T)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_rotated, y, \n",
    "                                                    test_size=0.20, \n",
    "                                                    random_state=random_state)\n",
    "\n",
    "f, ax = plt.subplots(nrows=1, ncols=1, figsize=(5, 5))\n",
    "plt.scatter(X_rotated[:, 0], X_rotated[:, 1], c=y, marker='o', s=12)\n",
    "plt.xlabel('X_1')\n",
    "plt.ylabel('X_2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eaVoa1J-m865"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "def visualize_decision_boundary(X, y, feature_new=None, h=0.02):\n",
    "    '''\n",
    "    You don't have to modify this function\n",
    "    \n",
    "    Function to vizualize decision boundary\n",
    "    \n",
    "    feature_new is a function to get X with additional features\n",
    "    '''\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx_1, xx_2 = np.meshgrid(np.arange(x1_min, x1_max, h),\n",
    "                         np.arange(x2_min, x2_max, h))\n",
    "\n",
    "    if X.shape[1] == 2:\n",
    "        Z = svm_cls.predict(np.c_[xx_1.ravel(), xx_2.ravel()])\n",
    "    else:\n",
    "        X_conc = np.c_[xx_1.ravel(), xx_2.ravel()]\n",
    "        X_new = feature_new(X_conc)\n",
    "        Z = svm_cls.predict(X_new)\n",
    "    Z = Z.reshape(xx_1.shape)\n",
    "    \n",
    "    f, ax = plt.subplots(nrows=1, ncols=1,figsize=(5,5))\n",
    "    plt.contourf(xx_1, xx_2, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
    "    plt.xlabel('X_1')\n",
    "    plt.ylabel('X_2')\n",
    "    plt.xlim(xx_1.min(), xx_1.max())\n",
    "    plt.ylim(xx_2.min(), xx_2.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "ZJfVwgE2m867",
    "outputId": "7f5cf1c7-bc5e-49d9-e8a4-1a38145d8b11"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "# Try to fit a linear classifier to the dataset\n",
    "\n",
    "svm_cls = svm.LinearSVC()\n",
    "svm_cls.fit(X_train, y_train)\n",
    "y_test_predicted = svm_cls.predict(X_test)\n",
    "\n",
    "print(\"Accuracy on test dataset: {}\".format(accuracy_score(y_test, \n",
    "                                                           y_test_predicted)))\n",
    "\n",
    "visualize_decision_boundary(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FuShu_ECm869"
   },
   "source": [
    "We can see that we need a non-linear boundary to be able to successfully classify data in this dataset. By mapping the current feature x to a higher space with more features, linear SVM could be performed on the features in the higher space to learn a non-linear decision boundary. In feature.py, modify create_nl_feature() to add additional features which can help classify in the above dataset. After creating the additional features use code in the further cells to see how well the features perform on the test set. \n",
    "\n",
    "**Note:** You should get a test accuracy above 85%\n",
    "\n",
    "**Hint:** Think of the shape of the decision boundary that would best separate the above points. What additional features could help map the linear boundary to the non-linear one? Look at [this](https://xavierbourretsicotte.github.io/Kernel_feature_map.html) for a detailed analysis of doing the same for points separable with a circular boundary\n",
    "\n",
    "<strong>TODO:</strong> Implement the <strong>create_nl_feature</strong> function in <strong>feature.py</strong>. There are many possible solutions to producing a decision boundary; think creatively!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SIwgaHVkMnPX"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "from feature import create_nl_feature\n",
    "\n",
    "X_new = create_nl_feature(X_rotated)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, \n",
    "                                                    test_size=0.20, \n",
    "                                                    random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "VaUahNX9m87B",
    "outputId": "c4624aea-2dd0-41ad-e844-1ecfdb1e8de9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "# Fit to the new features and vizualize the decision boundary\n",
    "# You should get more than 90% accuracy on test set\n",
    "\n",
    "svm_cls = svm.LinearSVC()\n",
    "svm_cls.fit(X_train, y_train)\n",
    "y_test_predicted = svm_cls.predict(X_test)\n",
    "\n",
    "print(\"Accuracy on test dataset: {}\".format(accuracy_score(y_test, y_test_predicted)))\n",
    "\n",
    "visualize_decision_boundary(X_train, y_train, create_nl_feature)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SUMMER2022_HW4_Solution.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "vscode": {
   "interpreter": {
    "hash": "b26bd32b0039b1654d06ab8599a5e3cb39f344cfa10a303934a1e3970c8fcff9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
